{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train classifier to estimate analysis efficiency vs gen variable\n",
    "\n",
    "- Actual training code in training.py\n",
    "- Classifier based on sklearn. Default is GradientBoostedClassifier, \n",
    "    but can be specified at run time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import train as tn\n",
    "reload(tn)\n",
    "\n",
    "import plotting\n",
    "reload(plotting)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.style.use('seaborn-ticks')\n",
    "plt.style.use('seaborn-poster')\n",
    "%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import os\n",
    "import json\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate helper class\n",
    "\n",
    "Data are read from ROOT trees and converted into pandas data frames.  \n",
    "The loading function makes sure that all the needed columns have been read from the trees, otherwise it rebilds the data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "params= {}\n",
    "# inputDir=\"/eos/user/m/musella/data/mod_dep_005\"\n",
    "params[\"dataDir\"]=\"root://t3dcachedb03.psi.ch//pnfs/psi.ch/cms/trivcat/store/user/musella/mod_dep_005\"\n",
    "params[\"dataFname\"] = \"output_InsideAcceptance_125.root\"\n",
    "params[\"pfx\"] = \"genDiphotonDumper/trees/InsideAcceptance_125_13TeV\"\n",
    "\n",
    "params[\"inputDir\"] = \".\"\n",
    "params[\"inputName\"] = \"effFitter\"\n",
    "params[\"outDir\"] = \".\"\n",
    "params[\"outName\"] = \"effFitter_out\"\n",
    "\n",
    "params[\"ncats\"] = 3\n",
    "params[\"genBranches\"] = [\"genPt\",\"genRapidity\",\n",
    "            \"genJet2p5Pt0\",\"genJet2p5Rapidity0\",\n",
    "            \"genJet2p5Pt1\",\"genJet2p5Rapidity1\",\n",
    "            \"genJet2p5Pt2\",\"genJet2p5Rapidity2\",\n",
    "            \"genJet2p5Pt3\",\"genJet2p5Rapidity3\",\n",
    "            \"weight\",\n",
    "            \"genNjets2p5\"\n",
    "           ]\n",
    "params[\"recoBranches\"] = ['recoPt','recoRapidity',\"recoNjets2p5\"]\n",
    "params[\"rndseed\"] = 9347865\n",
    "params[\"rndseed2\"] = 2315645\n",
    "\n",
    "params[\"split_frac\"] = 0.75\n",
    "#split_params = {\"train_size\" : 0.75, \"test_size\" : 0.25, \"random_state\" : rndseed2, \"stratify\" : False }\n",
    "\n",
    "params[\"load\"] = True\n",
    "params[\"forceMake\"] = False\n",
    "\n",
    "params[\"clean\"] = []\n",
    "\n",
    "params[\"classifiers\"] = [ \"class\", \"recoPt\", \"recoNjets2p5\" ]\n",
    "params[\"class\"] = [  \"sklearn.ensemble.GradientBoostingClassifier\", \n",
    "                      dict(trainevts=100000,max_depth=5,learning_rate=0.2,n_estimators=100,\n",
    "                          min_weight_fraction_leaf=1e-3)\n",
    "                    ]\n",
    "\n",
    "params[\"recoPt\"] = [ \"sklearn.ensemble.GradientBoostingClassifier\",\n",
    "                      dict(Xbr=['genPt','absGenRapidity'],\n",
    "                          trainevts=100000,max_depth=7,learning_rate=0.1,n_estimators=500,\n",
    "                          min_weight_fraction_leaf=1e-4)\n",
    "                    ]\n",
    "\n",
    "params[\"recoNjets2p5\"] = [ \"sklearn.ensemble.GradientBoostingClassifier\",\n",
    "                            dict(Xbr=['genJet2p5Pt0', 'genJet2p5Rapidity0',\n",
    "                                    'genJet2p5Pt1', 'genJet2p5Rapidity1',\n",
    "                                    'genJet2p5Pt2', 'genJet2p5Rapidity2',\n",
    "                                    'genJet2p5Pt3', 'genJet2p5Rapidity3',\n",
    "                                    'genPt','absGenRapidity'\n",
    "                                ],#factorized=True,\n",
    "                                trainevts=500000,max_depth=5,learning_rate=0.1,\n",
    "                                n_estimators=100,min_weight_fraction_leaf=1e-4,\n",
    "                                subsample=0.1,verbose=True)\n",
    "                         ]\n",
    "                      \n",
    "params[\"defineBins\"] = { 'recoPt' : dict(boundaries=[0,15,30,60,120,180,200]),\n",
    "                         'recoNjets2p5' : dict(boundaries=[-0.5,1.5,2.5,3.5,4.5])\n",
    "}\n",
    "\n",
    "\n",
    "params[\"clean\"] = [\"class\", \"recoPt\", \"recoNjets2p5\" ]\n",
    "params[\"class\"] = [  \"classify.BinnedFitter\", \n",
    "                      dict(trainevts=-1,bins=30,ranges=[(0,300),(0,30)]) ]\n",
    "\n",
    "params[\"recoPt\"] = [  \"classify.BinnedFitter\", \n",
    "                      dict(Xbr=['genPt','absGenRapidity'],includeClassProbs=False,\n",
    "                           trainevts=-1,bins=180,ranges=[(0,300),(0,30)],addprobs=True) ]\n",
    "\n",
    "\n",
    "params[\"classifiers\"] = [\"class\", \"recoPt\"]\n",
    "\n",
    "config_files = os.environ.get('my_train_config',None)\n",
    "if config_files:\n",
    "    for cfg in config_files.split(','):\n",
    "        print(\"reading %s\" % cfg)\n",
    "        with open(cfg) as fin: \n",
    "            loadparams = json.loads(fin.read())\n",
    "            params.update(loadparams)\n",
    "\n",
    "\n",
    "pprint(params)\n",
    "\n",
    "def runDefineBins(fitter,binsDef):\n",
    "    for name,params in binsDef.iteritems(): made.defineBins(name,**params)\n",
    "\n",
    "def loadOrMake():\n",
    "\n",
    "    name = params[\"inputName\"]\n",
    "    load = params[\"load\"]\n",
    "    forceMake = params[\"forceMake\"]\n",
    "    \n",
    "    make = False\n",
    "    if load:\n",
    "        onDisk = tn.IO.load(name, path=params[\"inputDir\"], nodata=forceMake)\n",
    "        pprint(onDisk)\n",
    "        if not forceMake:\n",
    "            pprint(onDisk.df.columns)\n",
    "        pprint(onDisk.clfs)\n",
    "        if onDisk.genBranches != params[\"genBranches\"] or onDisk.recoBranches != params[\"recoBranches\"]:\n",
    "            make = True\n",
    "        if onDisk.ncats != params[\"ncats\"]:\n",
    "            make = True\n",
    "            load = False\n",
    "    else:\n",
    "        make = True\n",
    "\n",
    "    if make or forceMake:\n",
    "        if not load:\n",
    "            made = tn.EfficiencyFitter(name)\n",
    "        else:\n",
    "            made = onDisk\n",
    "        \n",
    "        fileName = os.path.join(params[\"dataDir\"],params[\"dataFname\"])\n",
    "        made.readData(ncats,genBranches,recoBranches,[(fileName,None,pfx)])\n",
    "        \n",
    "        print('shuffling dataset')\n",
    "        np.random.seed(rndseed)\n",
    "        made.df['random_index'] = np.random.permutation(range(made.df.index.size))\n",
    "        made.df.sort_values(by='random_index',inplace=True)\n",
    "        made.df.set_index('random_index',inplace=True)\n",
    "        made.split_frac = split_frac\n",
    "        \n",
    "        print('defining bins')\n",
    "        if not 'absGenRapidity' in made.df.columns:\n",
    "            made.df['absGenRapidity'] = np.abs(made.df['genRapidity'])\n",
    "        runDefineBins(params[\"defineBins\"])\n",
    "        \n",
    "    else:\n",
    "        made = onDisk\n",
    "    \n",
    "    made.cleanClfs(params[\"clean\"])\n",
    "    \n",
    "    made.outdir = params[\"outDir\"]\n",
    "    made.name = params[\"outName\"]\n",
    "    return made\n",
    "\n",
    "effFitter = loadOrMake()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "effFitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Train classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#recoPts = filter(lambda x: \"recoPt_prob\" in x, effFitter.df.columns)\n",
    "#rename = { x : x.replace(\"_prob\",\"Cat_prob\") for x in recoPts }\n",
    "#effFitter.df.rename(columns=rename,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def naive_closure(df,column,first=0,logy=False):\n",
    "    target = target_name(column)\n",
    "    nstats = np.unique(df[target]).size\n",
    "    print(target,nstats)\n",
    "    \n",
    "    pred_cols = map(lambda x: (\"%s_prob_%d\" % (target, x)), range(nstats) ) \n",
    "    \n",
    "    trueh = np.histogram(df[target],np.arange(-1.5,nstats-0.5))[0].ravel()\n",
    "    predh = np.array(df[pred_cols].sum(axis=0)).ravel()\n",
    "    \n",
    "    print(trueh,predh)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    true = ax.bar(np.arange(0,2*(nstats),2)[first:],trueh[first:],color='black')\n",
    "    pred = ax.bar(np.arange(1,2*(nstats)+1,2)[first:],predh[first:],color='red')\n",
    "    if logy:\n",
    "        ax.set_yscale('log')\n",
    "        \n",
    "    plt.legend((true,pred),(\"true\",\"predicted\"))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def control_plots(key,fitter):\n",
    "    target = target_name(key)\n",
    "    \n",
    "    nclasses = len(fitter.clfs[key].classes_)\n",
    "    columns = map(lambda x: \"%s_prob_%d\" % (target,x), xrange(nclasses) )\n",
    "    \n",
    "    columns = columns[1:]+columns[:1]\n",
    "    \n",
    "    nrows = nclasses/3+1\n",
    "    ncols = 3\n",
    "    fitter.df.boxplot(by=target,column=columns,figsize=(7*ncols,7*nrows),layout=(nrows,ncols))\n",
    "    \n",
    "    plotting.scatter_hist(fitter.df,columns,figsize=(28,28))\n",
    "    \n",
    "    naive_closure(fitter.df,key,logy=True)\n",
    "    naive_closure(fitter.df,key,first=1,logy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  make sure that the trained classifers have been evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def target_name(key):\n",
    "    postFix = \"\"\n",
    "    if key != 'class':\n",
    "        postFix = 'Cat' if not params[key][1].get('factorized',False) else 'Bin'\n",
    "    return key+postFix \n",
    "\n",
    "clf_keys = filter(lambda x: x in effFitter.clfs.keys(),params[\"classifiers\"])\n",
    "     \n",
    "for key in clf_keys:\n",
    "    target = target_name(key)\n",
    "    catKey = '%s_prob_0' % (target)\n",
    "    if not catKey in effFitter.df.columns:\n",
    "        print('running prediction for %s' % key)\n",
    "        effFitter.runPrediction(target,effFitter.clfs[key])\n",
    "\n",
    "\n",
    "effFitter.df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "to_train = filter(lambda x: x not in effFitter.clfs.keys(), params[\"classifiers\"])\n",
    "\n",
    "for key in to_train:\n",
    "    classifier,train_params = params[key]\n",
    "    pack,cls = classifier.rsplit('.',1)\n",
    "    classifier = getattr(importlib.import_module(pack),cls)\n",
    "    print(\"Fitting %s\" % key)\n",
    "    print(classifier)\n",
    "    print(train_params)\n",
    "    if key == 'class':\n",
    "        %time effFitter.fitClass(classifier=classifier,**train_params)\n",
    "    else:\n",
    "        %time effFitter.fitBins(key,classifier=classifier,**train_params)\n",
    "    control_plots(key,effFitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pprint(effFitter.clfs)\n",
    "pprint(effFitter.df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tn.IO.save(effFitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
